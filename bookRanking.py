from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import pandas as pd
import time
import itertools

left_words = ["è€å¹´äºº", "è€äºº", "ä¸­è€å¹´"]
right_words = ["ä¹¦", "ä¹¦ç±", "è¯»ç‰©"]
keywords = [f"{l} {r}" for l, r in itertools.product(left_words, right_words)]

def crawl_titles(keywords, pages=5):
    options = Options()
    options.add_argument(r"user-data-dir=C:\Codes\python scripts\books\selenium_cache_jd")
    options.add_argument("--disable-gpu")
    options.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
        "(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
    )

    driver = webdriver.Chrome(options=options)
    driver.get("https://www.jd.com/")
    time.sleep(3)

    results = []

    for keyword in keywords:
        print(f"\nğŸ” å½“å‰å…³é”®è¯ï¼š{keyword}")
        for page in range(1, pages + 1):
            url = (
                f"https://search.jd.com/Search?keyword={keyword}&stock=1&psort=3&click=1&page={page * 2 - 1}"
            )
            print(f"ğŸ“„ ç¬¬ {page} é¡µï¼š{url}")
            driver.get(url)
            time.sleep(2)

            for _ in range(10):
                driver.execute_script("window.scrollBy(0, 1000);")
                time.sleep(0.5)

            items = driver.find_elements(By.CSS_SELECTOR, ".p-name em")

            if not items:
                print(f"âš ï¸ ç¬¬ {page} é¡µæ²¡æœ‰æ‰¾åˆ°ä¹¦åå…ƒç´ ï¼Œå¾ˆå¯èƒ½æ˜¯éªŒè¯é¡µé¢æˆ–é¡µé¢åŠ è½½å¤±è´¥")
                print("â¸ è¯·ä½ æ‰‹åŠ¨æŸ¥çœ‹æµè§ˆå™¨é¡µé¢æ˜¯å¦ä¸ºéªŒè¯é¡µï¼ˆå¦‚æ»‘åŠ¨éªŒè¯ï¼‰ï¼Œå®ŒæˆåæŒ‰å›è½¦ç»§ç»­å°è¯•æŠ“å–...")

                input("ğŸ‘‰ éªŒè¯å®ŒæˆåæŒ‰å›è½¦é‡è¯•å½“å‰é¡µï¼š")

                # é‡è¯•å½“å‰é¡µ
                driver.get(url)
                time.sleep(2)
                for _ in range(10):
                    driver.execute_script("window.scrollBy(0, 1000);")
                    time.sleep(0.5)

                items = driver.find_elements(By.CSS_SELECTOR, ".p-name em")
                if not items:
                    print("âŒ é‡è¯•åä»æœªæŠ“åˆ°ä¹¦åï¼Œè·³è¿‡è¯¥é¡µã€‚")
                    continue

            for idx, el in enumerate(items):
                try:
                    title = el.text.strip()
                    results.append({
                        "å…³é”®è¯": keyword,
                        "é¡µç ": page,
                        "åºå·": idx + 1,
                        "ä¹¦å": title
                    })
                except Exception:
                    continue

    driver.quit()
    return pd.DataFrame(results)

def compute_ranking(df, decay=0.85):
    df["score"] = 100 * (decay ** (df["é¡µç "] - 1)) * (1 - (df["åºå·"] - 1) / 30)
    df.to_csv("è€å¹´å›¾ä¹¦_åŒè¯ç»„åˆåˆ†æ.csv", index=False, encoding="utf-8-sig")

    grouped = df.groupby("ä¹¦å").agg(
        æ€»å¾—åˆ†=("score", "sum"),
        å‡ºç°æ¬¡æ•°=("score", "count"),
        è¦†ç›–å…³é”®è¯=("å…³é”®è¯", lambda x: ", ".join(sorted(set(x)))),
        æœ€æ—©é¡µç =("é¡µç ", "min"),
        æœ€æ—©åºå·=("åºå·", "min"),
    ).reset_index()

    grouped_sorted = grouped.sort_values(by="æ€»å¾—åˆ†", ascending=False)

    with open("è€å¹´å›¾ä¹¦_æ¨èæ’åº_0.85_decay_ç®€æ´ç‰ˆ.txt", "w", encoding="utf-8") as f:
        for i, row in enumerate(grouped_sorted.itertuples(index=False), 1):
            f.write(f"{i}.ã€Š{row.ä¹¦å}ã€‹ï¼Œç»¼åˆå¾—åˆ†ï¼š{row.æ€»å¾—åˆ†:.2f}\n")

    grouped_sorted.to_csv("è€å¹´å›¾ä¹¦_æ¨èæ’åº_0.85_decay.csv", index=False, encoding="utf-8-sig")

    print("\nâœ… æ’åºå·²å®Œæˆï¼è¾“å‡ºæ–‡ä»¶ï¼š")
    print("ğŸ“„ è€å¹´å›¾ä¹¦_åŒè¯ç»„åˆåˆ†æ.csv")
    print("ğŸ“„ è€å¹´å›¾ä¹¦_æ¨èæ’åº_0.85_decay.csv")
    print("ğŸ“„ è€å¹´å›¾ä¹¦_æ¨èæ’åº_0.85_decay_ç®€æ´ç‰ˆ.txt")

if __name__ == "__main__":
    raw_df = crawl_titles(keywords, pages=5)
    compute_ranking(raw_df, decay=0.85)
